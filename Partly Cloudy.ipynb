{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import utils as U\n",
    "reload(U)\n",
    "import image_loader as IM\n",
    "reload(IM)\n",
    "from theano.sandbox import cuda\n",
    "from keras.preprocessing import image\n",
    "%matplotlib inline\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, BatchNormalization, Input, Flatten, Conv2D, AveragePooling2D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mapping = U.binary_one_hot_mapping(\"partly_cloudy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "klass = \"partly_cloudy\"\n",
    "folder = \"data/train/\"\n",
    "train_folder = folder+\"train\"\n",
    "train_filenames = os.listdir(train_folder)\n",
    "validation_folder = folder+\"valid\"\n",
    "validation_filenames = os.listdir(validation_folder)\n",
    "batch_size = 64\n",
    "train_batch_count = len(train_filenames) / batch_size\n",
    "val_batch_count = len(validation_filenames) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.model' from 'models/model.pyc'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import models.model as M\n",
    "from keras.layers import Flatten, Dense, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "reload(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = M.full_model()\n",
    "for l in model.layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre compute convolutional filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_t = image.ImageDataGenerator(vertical_flip=True, horizontal_flip=True)\n",
    "\n",
    "gen = IM.DirectoryIterator(train_folder, train_filenames, image_processor=gen_t, one_hot_filename_mapping=mapping, batch_size=batch_size, shuffle=True)\n",
    "val_gen = IM.DirectoryIterator(validation_folder, validation_filenames, one_hot_filename_mapping=mapping, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7fb067618472>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maugmented_conv_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_batch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maugmented_val_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batch_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_model' is not defined"
     ]
    }
   ],
   "source": [
    "augmented_conv_features = conv_model.predict_generator(gen, train_batch_count, verbose=1)\n",
    "augmented_val_features = conv_model.predict_generator(val_gen, val_batch_count, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#U.save_array(\"precomputed-conv-features\", augmented_conv_features)\n",
    "#U.save_array(\"precomputed-conv-val-features\", augmented_val_features)\n",
    "augmented_conv_features = U.load_array(\"precomputed-conv-features\")\n",
    "augmented_val_features = U.load_array(\"precomputed-conv-val-features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Dense Layers, Tuning Dropout and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import binary_one_hot_mapping\n",
    "mapping = binary_one_hot_mapping(klass)\n",
    "\n",
    "train_labels = [ mapping[fname] for fname in train_filenames ]\n",
    "valid_labels = [ mapping[fname] for fname in validation_filenames ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "    * Binary crossetropy on Dense(1) vs Categorical on Dense(2)?\n",
    "    * Can we weight loss differently for different categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_layers = [\n",
    "    Flatten(input_shape=(8,8,512)),\n",
    "    BatchNormalization(),\n",
    "    Dense(4096, activation=\"relu\", kernel_regularizer=l2(0.005)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(4096, activation=\"relu\", kernel_regularizer=l2(0.005)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(2, activation=\"softmax\"),\n",
    "]\n",
    "\n",
    "dense_model = Sequential(dense_layers)\n",
    "conv_model = model\n",
    "#model = Sequential(model.layers + dense_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dense_model.load_weights(\"weights/dense_weights_partly_cloudy_2\")\n",
    "dense_model.compile(optimizer=(Adam(lr=0.0001)), metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_model.fit(augmented_conv_features, train_labels, batch_size=batch_size, epochs=2, validation_data=(augmented_val_features, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_model.load_weights(\"weights/dense_weights_partly_cloudy\")\n",
    "#dense_model.optimizer.lr = 0.00001\n",
    "#dense_model.fit(augmented_conv_features, train_labels, batch_size=batch_size, epochs=1, validation_data=(augmented_val_features, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dense_model.save_weights(\"weights/dense_weights_partly_cloudy_2\")\n",
    "dense_model.optimizer.lr = 0.00001\n",
    "dense_model.fit(augmented_conv_features, train_labels, batch_size=batch_size, epochs=2, validation_data=(augmented_val_features, valid_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dense_model.load_weights(\"weights/dense_weights_partly_cloudy_2\")\n",
    "\n",
    "dense_layers = [\n",
    "    Flatten(input_shape=(8,8,512)),\n",
    "    BatchNormalization(name=\"batch_norm_1\"),\n",
    "    Dense(4096, activation=\"relu\", name=\"Dense 1\"),\n",
    "    BatchNormalization(name=\"batch_norm_2\"),\n",
    "    Dropout(0.0),\n",
    "    Dense(4096, activation=\"relu\", name=\"Dense 2\"),\n",
    "    BatchNormalization(name=\"batch_norm_3\"),\n",
    "    Dropout(0.0),\n",
    "    Dense(2, activation=\"softmax\", name=\"Dense Output Layer\"),\n",
    "]\n",
    "\n",
    "m = Sequential(dense_layers)\n",
    "m.load_weights(\"weights/dense_weights_partly_cloudy_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layers = M.vgg().layers\n",
    "full_model = Sequential(conv_layers + dense_layers)\n",
    "\n",
    "full_model.save_weights(\"weights/partly_cloudy_full_model_top_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "klass = \"partly_cloudy\"\n",
    "folder = \"data/samples/partly_cloudy/\"\n",
    "train_folder = folder+\"train\"\n",
    "train_filenames = os.listdir(train_folder)\n",
    "validation_folder = folder+\"valid\"\n",
    "validation_filenames = os.listdir(validation_folder)\n",
    "batch_size = 64\n",
    "train_batch_count = len(train_filenames) / batch_size\n",
    "val_batch_count = len(validation_filenames) / batch_size\n",
    "gen = IM.DirectoryIterator(train_folder, train_filenames, one_hot_filename_mapping=mapping, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for l in full_model.layers:\n",
    "    l.trainable = False\n",
    "\n",
    "full_model.compile(optimizer=(Adam(lr=0.0001)), metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
    "preds = full_model.fit_generator(gen, train_batch_count, validation_data=val_gen, validation_steps=val_batch_count)\n",
    "\n",
    "res = zip(train_filenames, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss: 1.6267 - acc: 0.7344 - val_loss: 0.6926 - val_acc: 0.8424\n",
    "wrong = U.incorrect_predictions(res, mapping)\n",
    "print(res[0])\n",
    "mapping['train_28750.jpg']\n",
    "from matplotlib import pyplot as plt\n",
    "img = IM.load_to_numpy(\"data/train-jpg/\"+wrong[0][0])\n",
    "gen_t = image.ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "img = gen_t.random_transform(img)\n",
    "plt.imshow(img)\n",
    "#U.plot(\"data/train-jpg/\"+wrong[0][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Full Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, BaseLogger, History, CSVLogger\n",
    "callbacks = [\n",
    "    History(),\n",
    "    BaseLogger(),\n",
    "    ModelCheckpoint(\"weights/partly-cloudy-full-training.{epoch:02d}-{val_loss:.2f}.hdf5\", monitor=\"val_loss\", verbose=0, save_best_only=False, save_weights_only=True, mode='auto', period=1),\n",
    "    CSVLogger(\"results/partly_cloudy.csv\", separator=',', append=False)\n",
    "]\n",
    "\n",
    "dense_layers = [\n",
    "    Flatten(input_shape=(8,8,512)),\n",
    "    BatchNormalization(),\n",
    "    Dense(4096, activation=\"relu\", kernel_regularizer=l2(0.005)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(4096, activation=\"relu\", kernel_regularizer=l2(0.005)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(2, activation=\"softmax\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layers = M.vgg().layers\n",
    "full_model = Sequential(conv_layers + dense_layers)\n",
    "#full_model.load_weights(\"weights/partly_cloudy_full_model_top_trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1088/1088 [==============================] - 5886s - loss: 0.6984 - acc: 0.9501 - val_loss: 0.5514 - val_acc: 0.9558\n",
      "Epoch 2/2\n",
      "1088/1088 [==============================] - 5886s - loss: 0.4640 - acc: 0.9611 - val_loss: 0.3912 - val_acc: 0.9641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5bf98ef90>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "klass = \"partly_cloudy\"\n",
    "folder = \"data/train/\"\n",
    "train_folder = folder+\"train\"\n",
    "train_filenames = os.listdir(train_folder)\n",
    "validation_folder = folder+\"valid\"\n",
    "validation_filenames = os.listdir(validation_folder)\n",
    "\n",
    "train_batch_count = len(train_filenames) / batch_size\n",
    "val_batch_count = len(validation_filenames) / batch_size\n",
    "\n",
    "gen_t = image.ImageDataGenerator(vertical_flip=True, horizontal_flip=True)\n",
    "\n",
    "gen = IM.DirectoryIterator(train_folder, train_filenames, image_processor=gen_t, one_hot_filename_mapping=mapping, batch_size=batch_size, shuffle=True)\n",
    "val_gen = IM.DirectoryIterator(validation_folder, validation_filenames, one_hot_filename_mapping=mapping, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for l in full_model.layers:\n",
    "    l.trainable = True\n",
    "\n",
    "full_model.compile(optimizer=(Adam(lr=0.00001)), metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")\n",
    "full_model.fit_generator(gen, train_batch_count, epochs=2, validation_data=val_gen, validation_steps=val_batch_count , callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1088/1088 [==============================] - 5877s - loss: 0.3479 - acc: 0.9632 - val_loss: 0.3196 - val_acc: 0.9570\n",
      "Epoch 2/2\n",
      "1088/1088 [==============================] - 5868s - loss: 0.2808 - acc: 0.9641 - val_loss: 0.2531 - val_acc: 0.9675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb5cf7287d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.fit_generator(gen, train_batch_count, epochs=2, validation_data=val_gen, validation_steps=val_batch_count , callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_model.optimizer.lr = 0.000001\n",
    "full_model.fit_generator(gen, train_batch_count, epochs=10, validation_data=val_gen, validation_steps=val_batch_count , callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
